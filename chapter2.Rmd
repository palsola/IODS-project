# 2. Regression analysis

In this chapter I will: 

- Introduce the data set
- Describe the use of regression analysis as a method.
- Describe the results obtained from regression analysis.
- Summarize what I learned in this exercise.

The preceding data wrangling exercise is available <a href="https://github.com/palsola/IODS-project/blob/master/data/create_learning2014.R"> here<a/>.

<font size=-1 color=#7A7A7A>*I will assume that the reader has an introductory course level understanding of writing and reading R code as well as statistical methods, and that he/she has no previous knowledge of your data or the more advanced methods you are using.*</font>


Here we go again...

## Description of data

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

learning2014 <- read.table("C:/Users/minttu/R projects/IODS-project/data/Learning 2014.txt", header = T)

# explore the structure and the dimensions of the data
dim(learning2014)
str(learning2014)

```


The dataset used here is an extract from a wider dataset (N=183) collected on the *Introduction to Social Statistics* course during 3.12.2014 - 10.1.2015 by Kimmo Vehkalahti. The international survey of Approaches to Learning was used for data collection. The current dataset used for this excercise has `r nrow(learning2014)` observations of `r ncol(learning2014)` variables: gender (M/F), age (in years), global attitude towards statistics (mean sumscore of 10 items), 3 learning type mean sumscore variables (deep; 12 items, strategic; 8 items, and surface; 12 items), and exam score (points). 

A more detailed summary of the dataset below:
```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

summary(learning2014)

```

### Graphic overview

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

# access libraries
library(ggplot2)
library(GGally)

# graphic overview

graphic_overview <- ggpairs(learning2014, 
             mapping = aes(col = gender, alpha = 0.3), 
             upper = list(continuous = wrap("cor", size = 3, alignPercent = 1)),
             lower = list(combo = wrap("facethist", bins = 30)))

graphic_overview

```

In this data, there are more females than males. The majority of the participants are in their 20s (distribution skewed to left). The distributions look similar for both genders except for attitude, where more femailes have given lower ratings, and surface learning where females have a higher peak. Distributions are relatively normal (slight skews to right), but the points variable has a notably a fat right-hand tail. Some outliers are detected for age (older participants), attitude (low scores among males), deep learning (low scores for both) and points (low score among females).

Overall, the highest positive correlations can be found between attitude and points (r>0.4 for both genders), and negative correlations between deep and surface learning (r=-0.324) due to higher correlation among males (r=-0.622) than females (r=0.087). Interestingly, attitude and surface learning have higher negative correlations among females (r=-0.374) compared to males (r=-0.0148). Otherwise correlations are quite low.

Scatter plots seem quite evenly spread - except for plots paired with age as most respondents are at the younger end.

*<font color=#7A7A7A size=-1> **Side note**: If someone knows how to spread out the figure a little more in order to better see the values at the bottom, let me know!</font>*


## Regression model

To investigate what (combination of) the variables in the data could explain the level of success in the course exam, I am conducting a multivariate linear regression analysis.  Regression analysis allow assessments of the relationship multiple independent variable sand a dependent variable. In multivariate regression models it is possible to have more than one independent variable which allows, e.g., controlling for possible confounding factors. The optimal set of independent variables is the smallest reliable uncorrelated set that explains enough of the variance in the dependent variable.

I first used attitude, strategic learning and deep learning as independent variables, but excluded both strategic and deep learning from the final model since they were not statistically significantly associated with the achieved exam points (as indicated by the p values above the 0.05 level and confidence intervals including zero).  For the sake of the exercise, I tried plotting all the other variables as well but none of them reached statistical significance.

Hence, the final model is as follows:

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# regression model and summary
my_model <- lm(points ~ attitude, data = learning2014)
summary(my_model)

confint(my_model, level = 0.95)

```

The total variance explained by the model for autonomous motivation was 19,06%, F(1, 164)=19.06, p<.001. This indicates, that there are many other factors that are associated with the exam score not captured by this model. This, however, is often the case in predicting human behavior and it is understandable that mere attitude cannot fully explain success in an exam.

For a given predictor variable, the coefficient (Estimate) can be interpreted as the average effect on exam scores of a one unit increase in predictor, holding all other predictors fixed. So, a 1-point increase in attitude (on a scale from 1-5) will, on average, increase points achieved in the exam by 3.53.



## Diagnosic plots

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}

# diagnostic plots
par(mfrow = c(2,2))
plot(my_model, which = c(1,2,5),
     caption = list("Residuals vs Fitted values", "Normal QQ-plot", "Residuals vs. Leverage"))

```


Multivariate regression assumes independence of the residuals, that is, normality, linearity, and homoscedasticity. 

It is assumed that: 

1. The errors are normally distributed
2. The errors are not correlated
3. The errors have constant variance 
4. The size of a given error does not depend on the size of explanatory variables

The assumption of constant variance can be explored from the scatter plot where any pattern implies a violation of the assumption. Here, the pattern seems otherwise relatively random, but some clustering can be detected at the lower right-hand corner of the chart. 

QQ- plot of the residuals provides a method to explore the assumption that the errors of the model are normally distributed. Here, the points fall nicely on the line in the middle, but some deviations can be detected at both ends, indicating a possible violation of this assumption.

Leverage measures how much impact a single observation has on the model.If cases are outside of the Cookâ€™s distance (dashed line), the cases are influential to the regression results. Here, we see that there are observations that lie outside that mark, thus influencing the model excessively.

For this model to better predict the exam scores, the more influential values and outliers should either be deleted or given a score that is not too far from the rest.

## Learning points

In this exercise I learned how to visualize data, conduct a simple regression analysis and create diagnostic plots. I have conducted regression analysis using SPSS before, so the method itself was not new. However, SPSS has only limited selection of diagnostic tools, and I had never seen nor analyzed a Leverage vs. Residuals scatter plot - so that was completely new to me! 

In terms of using R, everything is always new! So this week I learned many useful commands to get started with basic analyses and built a good foundation to move on to more refined analyses.
