# 4. Clustering and classification


## Data exploration

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}

# Load the Boston data from the MASS package

library(MASS)
data("Boston")

# Explore the structure and the dimensions
str(Boston)
summary(Boston)

```
The Housing Values in Suburbs of Boston data has `r nrow(Boston)` observations of `r ncol(Boston)`. The data includes measures of different aspects of housing  e.g., house values, crime rates, pollution, accessibility to highways and business centers, taxes. schools, etc. (see full list of measures <a href="https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html">here</a>.)

Most measures are numeric, except for Charles river dummy variable (chas) and accessibility to radial highways (rad, scale 1-24). A brief summary of the data is presented above. 

```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=FALSE}
pairs(Boston)


```

The graphical overview presented above has a lot of information but is difficult to understand and interpret. As associations between the variables are hard to see in this figure, I will use a correlation plot to look at the associations in the data. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library(corrplot)
library(tidyverse)
library(dplyr)

cor_matrix<-cor(Boston) %>% round(digits = 2)
cor_matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos= "b", tl.pos = "d", tl.cex = 0.6)

```

In the figure above, red circles indicate negative correlations and blue positive. The bigger and darker the circle, the higher the correlation.

High positive correlations, i.e., big blue circles, can be found between e.g., accessibility to radial highways (rad) vs. property-tax rate (tax).

Respectively, high negative correlations, i.e., big red circles, can be found between e.g., proportion of non-retail business acres (indus) and distances to  Boston employment centres (dis), nitrogen oxides concentration (nox) and distances to Boston employment centres, and proportion of owner-occupied units built prior to 1940 (age) and distances to  Boston employment centres.

## Data standardization and preparation for analyses

Next, I will scale the data by subtracting the column means from the corresponding columns and divide the difference with standard deviation.

```{r echo=TRUE, message=FALSE, warning=FALSE}
boston_scaled <- scale(Boston)
boston_scaled <- as.data.frame(boston_scaled)
summary(boston_scaled)

```
Now all the variables have a mean of 0.000, and more comparable ranges compared to the original data.

Next, I will create a categorical variable of the crime rate from the scaled crime rate using quantiles as the break points, and replace the original crime rate with the new categorical variable.

```{r echo=TRUE, message=FALSE, warning=FALSE}

bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))

boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)

```
Finally, I will divide the dataset to train and test sets, so that 80% of the data belongs to the train set.

```{r}
n <- nrow(boston_scaled)
ind <- sample(n,  size = n * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]

# check that % looks ok
dim(train)
dim(test)
```

## Linear discriminant analysis and visualization

Next, I will fit the linear discriminant analysis on the train set using the categorical crime rate as the target variable and all the other variables in the dataset as predictor variables.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# fit analysis


lda.fit <- lda(crime ~ ., data = train)
lda.fit

#draw model
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

classes <- as.numeric(train$crime)

plot(lda.fit, dimen = 2, col= classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)


```

In this two-dimensional figure, high is relatively distinguishable from the rest - even though that there are a few med_highs in the cluster.The rest of the classes are quite crammed together.




## Predicting classes with the LDA model on the test data

Now I will predict the classes with the LDA model on the test data and cross tabulate the results with the crime categories from the test set. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# save the crime categories from the test set
correct_classes <- test$crime

# remove the categorical crime variable from the test dataset
test <- dplyr::select(test, -crime)

# predict the classes with the LDA model on the test data

lda.pred <- predict(lda.fit, newdata = test)
table(correct = correct_classes, predicted = lda.pred$class)


```
No class was perfectly predicted, but it predicts higher crime rates better than lower. In the cross-tabulation the predictions get better the further down the rows we go. 


## K-means clustering

Reload the Boston dataset and standardize the dataset. Calculate the distances between the observations. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
library (MASS)
data ("Boston")
boston_scaled1 <- scale(Boston)
boston_scaled1 <- as.data.frame(boston_scaled1)

dist_eu <- dist(boston_scaled1)
dist_man <- dist(boston_scaled1, method = 'manhattan')
summary(dist_eu)
summary(dist_man)

```

Run k-means algorithm on the dataset. Visualize the clusters (for example with the pairs() or ggpairs() functions, where the clusters are separated with colors) and interpret the results. 

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Run k-means on the dataset.
km <- kmeans(boston_scaled1, centers=4)

# visualize
pairs(boston_scaled1, col = km$cluster)

# cannot see a thing, zooming into last five variables
pairs(boston_scaled1 [9:14], col = km$cluster)
````

Investigate what is the optimal number of clusters and run the algorithm again. 

```{r}
library(ggplot2)
set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled1, k)$tot.withinss})
qplot(x = 1:k_max, y = twcss, geom = 'line')
```

Based on the graph above, the optimal number of clusters is about two because the total WCSS notably changes direction around in that point. The K-means clustering is thus re-performed with two clusters.

```{r echo=TRUE, message=FALSE, warning=FALSE}
# Re-running k-means
km1 <- kmeans(boston_scaled1, centers=2)

# visualize
pairs(boston_scaled1, col = km$cluster)

# still cannot see a thing, zooming into five variables
pairs(boston_scaled1 [5:9], col = km$cluster)
````
